# Codebase: Improving performance for SemEval2019 with modern NLP techniques

This project is the codebase of the research 'Improving performance for SemEval2019 with modern NLP techniques' of November 2023 at the University of Groningen. Aimed at classifying of offensive tweets. Both, concerning classic models and more advanced models.

How to download all required data and packages can be found in section [Run](#run)

## File structure

```
CODEBASE
|-- data
|   |-- dev.tsv
|   |-- GPT.csv
|   |-- test.tsv
|   |-- train.tsv
|-- helpers
|   |-- helpers_baseline.py
|   |-- helpers_general.py
|   |-- helpers_pretrained.py
|   |-- helpers_lstm.py
|-- gpt
|   |-- app.py
|   |-- ...
|-- results
|   |-- ...
|-- .gitignore
|-- requirements.txt
|-- run_baseline.py
|-- run_lstm.py
|-- run_pretrained.py
```

### data

The data directory consists of four data files. All train, dev and test data is contained in the corresponding file. Examples which are generated by GPT are stored in `GPT.csv`. This file contains the samples which are used to obtain the reported results.

Any results will be saved in the `results` directory.

### helpers

In total there exist four helper files. The helper files consist of code needed by the corresponding models. `helpers_general` consists of code that is used by two or more models. The functions are called when running one of the models and can not be ran independently.

### gpt

Contains the code for generating offensive and non-offensive tweets and the code for zero-shot classifying tweets. This needs an `OPENAI_API_KEY` and credits to work.

### models

Various are provided. Divided into three separate files. Classic models with and without POS can be found in `run_baseline.py`. LSTM model can be found in `run_lstm.py`. And BERT models can be found in `run_pretrained.py`.

#### run_baseline

Firstly, the data will be undersampled after which it will train and test each model for each n-gram range, the ranges to be tested can be defined.
Which models should be tested can be selected in the function `run_range`. The array `classifiers` contains all tested classifiers, KNN and SVM. If you do not want to run all classifiers, it is possible to comment out some of them.

If wanted to run with POS, please use the corresponding flag. All possible flags:

```
--train_file {-> Input file to learn from
--dev_file {-> Separate dev set to read in
--test_file {-> If added, use trained model to predict on test set
--results_dir {-> Where to store results
--part_of_speech {-> Define whether to use POS
--rangestart {-> Which minimum n-gram to test
--rangeend {-> Which maximum n-gram to test
--min_df {-> Minimum appearance of n-grams
--stem {-> Add flag if should be stemmed
--lemmatize {-> Add flag if should be lemmatized
--emoji_remove {-> Add flag if emojis should be removed
```

#### run_lstm

Firstly, the data will be undersampled after which it will train and test each model. Moreover, multiple tests can be ran with the arguments passed to the program when running `python run_lstm.py`. Models can be tested for trainable and dense layers, bidirectional, multiple layers, etc.

All possible flags:
```
--train_file {-> Input file to learn from
--dev_file {-> Separate dev set to read in
--test_file {-> Use trained model to predict on test set
--lemmatize {-> Whether to lemmatize input
--stem {-> Whether to stem input
--emoji_remove {-> Whether to remove emojis
--embeddings {-> Whether to add embeddings or not
--trainable {-> Whether to add a trainable layer
--add_dense {-> Whether to add a dense layer
--add_layer {-> Whether to add an extra LSTM layer
--dropout {-> How much dropout should be used
--recurrent_dropout {-> How much recurrent dropout should be used
--bidirectional {-> If the LSTM layers should be bidirectional
--learning_rate {-> The learning rate
--batch {-> The bath size
```
#### run_pretrained

Firstly, the data will be undersampled after which it will train and test each model.
Which pretrained models should be tested can be selected in the main function. The dict `pretrained` contains all tested models. If you do not want to run all models, it is possible to comment out some of them.

All possible flags:

```
--train_file {-> Input file to learn from
--dev_file {-> Separate dev set to read in
--test_file {-> If added, use trained model to predict on test set
--gpt_file {-> If added, use gpt generated samples to train
--results_dir {-> Where to store results
--stem {-> Add flag if should be stemmed
--lemmatize {-> Add flag if should be lemmatized
--emoji_remove {-> Add flag if emojis should be removed
--epochs {-> Set number of epochs
--batch {-> Set batch size
--startrate {-> Set start of polynomial learning rate
--endrate {-> Set end of polynomial learning rate
--seqlen {-> Set sequence length
```

#### evaluate
This script makes it easy to use fine-tuned models for inference on other datasets. Give it an input file and output file and a fine-tuned model and it will provide the results in the desired output folder.


All possible flags:

```
--input_file {-> Input file to do inference on
--output_file {-> To put the results in
--model {-> Model repository
--lemmatize {-> Apply lemmatization
--stem {-> Apply stemming
--emoji_remove {-> Apply emoji removal
```
## Run

### Pre requisites
Before being able to run the codebase you will need to do the following:
1. Create a new pip/conda environment for `Python3.10`
2. `pip install -r requirements.txt`
3. `python -m spacy download en_core_web_sm`
4. (Optional) unzip the glove embeddings file to run `run_lstm.py`
5. Now you can run the `run_*.py` and `evaluate.py` files!

### Run

The models can be run via their corresponding run file, add flags if wanted. For instance for the baseline with part of speech tagging.

```
python3 run_baseline.py --part_of_speech
```

The data will be taken from the data directory and results are stored in the results directory.
